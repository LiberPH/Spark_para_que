{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark, ¿para qué?\n",
    "---\n",
    "\n",
    "### por Libertad Pantoja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" alt=\"drawing\" width=\"300\">\n",
    "\n",
    "# LiberPH\n",
    "\n",
    "## https://github.com/LiberPH/Spark_para_que"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "1. Comunicar una posible estrategia a seguir cuando tenemos demasiados datos.\n",
    "2. Dar a conocer qué es Spark.\n",
    "3. Mostrar algunos usos sencillos de Spark junto con algunas recomendaciones de cómo usarlo.\n",
    "4. Errores o detalles comunes.\n",
    "\n",
    "Nos vamos a enfocar en los aspectos prácticos :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datos, datos, datos\n",
    "___\n",
    "Imaginemos que queremos hacer el análisis de ciertos datos. Ya hemos trabajado antes con ciertos volúmenes de datos (digamos tablas de unos 100-10,000 datos) haciendo análisis exploratorios en Python.\n",
    "\n",
    "<img src=\"https://www.andrewheiss.com/blog/2019/01/29/diff-means-half-dozen-ways/eda-plots-1.png\" alt=\"drawing\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datos, datos, datos\n",
    "___\n",
    "Supongamos que incluso estamos familiarizados con algunas paqueterías de Python para hacer ciencia de datos como Scikit learn, llegamos a un punto en que nos parecen intuitivas o cómodas.\n",
    "\n",
    "\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/01/scikit-learn-logo.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pero...\n",
    "---\n",
    "\n",
    "De pronto entramos a un proyecto donde se requiere analizar más de 5Gb de datos. \n",
    "\n",
    "\n",
    "<img src=\"https://predictiontechnology.ucla.edu/wp-content/uploads/2016/05/BD.jpg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "¿Puedo hacerlo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué tanto es tantito?\n",
    "---\n",
    "¿Puedo hacerlo?\n",
    "\n",
    "Sí, en tanto nuestra computadora lo permita. La posibilidad de analizar un set de datos va a depender de nuestra capacidad de cómputo.\n",
    "\n",
    "\n",
    "<img src=\"https://static.thenounproject.com/png/2775987-200.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Scikit book](https://images-na.ssl-images-amazon.com/images/I/71P4atQpTbL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué hago si mi computadora no puede ejecutar los comandos que conozco con estos datos?\n",
    "---\n",
    "<img src=\"https://i1.sndcdn.com/artworks-000223795050-2qhpbr-t500x500.jpg\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Puedo conseguir una computadora con más capacidad de cómputo... \n",
    "\n",
    "...o ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Divide y vencerás\n",
    "___\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-b94eb7113c92d6101462bb5dc9bd79dd\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Características del Big Data\n",
    "---\n",
    "\n",
    "<img src=\"https://www.edureka.co/blog/wp-content/uploads/2018/06/Five-Vs-of-Big-Data-What-is-Big-Data-Edureka.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¿De qué trata esta plática?\n",
    "\n",
    "1. ¿Cómo se dividen las tareas?\n",
    "2. ¿Qué es Spark?\n",
    "3. ¿Cómo se usa Spark?\n",
    "4. Errores o detalles comunes\n",
    "\n",
    "Nos vamos a enfocar en los aspectos prácticos :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. ¿Cómo se dividen las tareas?\n",
    "---\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/3136/1*eLdh98v51Yi3AyNI1idfpA.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. ¿Cómo se dividen las tareas?\n",
    "---\n",
    "* Las tareas se dividen (distribuyen) entre las computadoras que forman parte del clúster por medio del clúster manager (dependiendo de los comandos utilizados).\n",
    "* Algunas de estas tareas se estarán ejecutando al mismo tiempo (paralelas) lo que reduce el tiempo total del proceso cuando trabajamos con grandes cantidades de datos.\n",
    "\n",
    "\n",
    "![Spark](https://raw.githubusercontent.com/LiberPH/Spark_para_que/main/Software_org_spark_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. ¿Qué es Spark?\n",
    "\"Unified analytics engine for large-scale data processing\"\n",
    "<img src=\"https://miro.medium.com/max/1400/0*6rCBTaD7N99u8qRT\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. ¿Qué es Spark?\n",
    "Apache Spark is an **open-source**, **distributed** processing system used for **big data** workloads. It utilizes in-memory caching and optimized query execution for fast queries against data of any size. Simply put, Spark is a fast and general engine for large-scale data processing.\n",
    "\n",
    "Es una herramienta que se puede ejecutar sobre Python, R, Scala o Java y permite trabajar con Big Data.\n",
    "\n",
    "https://chartio.com/learn/data-analytics/what-is-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entonces...\n",
    "### Spark es:\n",
    "* Distribuido\n",
    "* Open Source\n",
    "* Se usa para big data\n",
    "\n",
    "### Pero, además es:\n",
    "* Resiliente\n",
    "* Lazy\n",
    "* Sencillo de usar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. ¿Qué es Spark?\n",
    "---\n",
    "<img src='https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2017/05/features-of-spark.jpg' alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "https://data-flair.training/blogs/apache-spark-features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. ¿Qué es Spark?\n",
    "---\n",
    "El **driver** (master) controla todo lo que hacen las demás computadoras (**workers**). Pero los workers (slaves) hacen todo el trabajo.\n",
    "\n",
    "* Los **RDDs** (la estructura de datos principal de Spark) están particionados a través de los workers.\n",
    "* Los **workers** administran estas particiones y sus recursos. Se comunica con el clúster manager para decirle qué tanto espacio queda y para qué tareas.\n",
    "* Los **ejecutores** ejecutan tareas en su partición.Estas tareas llegan de tu programa a través del spark context.\n",
    "* **Caché** es la memoria que está siendo usada para guardar RDD's (memoria local en el nodo esclavo).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![Softspark](https://raw.githubusercontent.com/LiberPH/Spark_para_que/main/software_spark.png) \n",
    "https://www.edx.org/course/big-data-analytics-using-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce\n",
    "---\n",
    "\n",
    "Spark se basa en una aproximación divide y vencerás llamada MapReduce. MapReduce incluye otras cosas además de la idea de dividir un problema en tareas simples y después conjuntar un problema. Por ejemplo:\n",
    "\n",
    "* Guardado de estados intermedios\n",
    "* Identificación de nodos muertos\n",
    "* Etiquetado de resultados para indicar qué resultados van con cuáles.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/199Q1.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. ¿Cómo se usa Spark?\n",
    "---\n",
    "\n",
    "Lo primero que necesitamos para poder usar Spark es definir su contexto (Spark Context).\n",
    "\n",
    "\n",
    "![sc](https://raw.githubusercontent.com/LiberPH/Spark_para_que/main/spark_context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inicializando el Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://633d067428e9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vamos a leer datos\n",
    "---\n",
    "\n",
    "Pueden ser:\n",
    "\n",
    "* json\n",
    "\n",
    "* csv\n",
    "\n",
    "* parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Antes de continuar...\n",
    "* Nos vamos a enfocar en Spark DataFrames (también existen RDDs y Data sets) \n",
    "\n",
    "\n",
    "### Recomendación: Empieza a usar PySpark (o Python) con Jupyter Notebooks\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Jupyter_logo.svg/1200px-Jupyter_logo.svg.png' alt=\"drawing\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una forma sencilla es bajando un contenedor docker que ya trae Spark y Python. \n",
    "\n",
    "Contenedor recomendado: https://hub.docker.com/r/jupyter/all-spark-notebook\n",
    "\n",
    "#### Para usarlo: \n",
    "1. Hay que bajar Docker. https://www.docker.com/products\n",
    "2. Hay que descargar el contenedor: docker pull jupyter/all-spark-notebook <=En terminal\n",
    "3. Hay que ejecutar el contenedor: docker run -it --rm -p 8888:8888 jupyter/all-spark-notebook <=En terminal\n",
    "4. Es necesario usar el url desplegado en la terminal (pégalo en tu navegador)\n",
    "\n",
    "<img src='https://webstockreview.net/images/surprise-clipart-woe-4.png' alt=\"drawing\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src='https://i.stack.imgur.com/3rF6p.png' alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Usar DataFrames proporciona varias ventajas:\n",
    "---\n",
    "\n",
    "* Sintaxis más simple que la de los RDDs\n",
    "* Posibilidad de usar comandos tipo SQL directamente sobre el DataFrame\n",
    "* Operaciones están automáticamente distribuidas\n",
    "La principal ventaja de usar los DataFrames de Spark sobre Pandas o cualquier otra cosa es que Spark puede manipular  cantidades masivas de datos que no caben en una sola computadora. El costo por esto es una sintaxis un poquito rara, pero nada terrible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Operaciones en Spark\n",
    "\n",
    "* **Transformaciones:** Son operaciones que dan como resultado un nuevo RDD. Son *lazy* ya que no se ejecutan sino hasta que tiene lugar una acción.\n",
    "* **Acciones:** Son operaciones que dan como resultado una variable local (número, string, etc). Al ejecutarlas desencadenan la serie de transformaciones previa a poder calcularla.\n",
    "\n",
    "<img src='https://miro.medium.com/max/726/1*BaQ7kuuENGOWbV7JII0gEA.png' alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ahora sí, ¡a leer!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# https://www.kaggle.com/johnsmith88/heart-disease-dataset\n",
    "# Puede que tome un tiempo en una computadora local\n",
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv('heart.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Heart\n",
    "\n",
    "\n",
    "1. age\n",
    "2. sex\n",
    "3. chest pain type (4 values)\n",
    "4. resting blood pressure\n",
    "5. serum cholestoral in mg/dl\n",
    "6. fasting blood sugar > 120 mg/dl\n",
    "7. resting electrocardiographic results (values 0,1,2)\n",
    "8. maximum heart rate achieved\n",
    "9. exercise induced angina\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. the slope of the peak exercise ST segment\n",
    "12. number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 0 = normal; 1 = fixed defect; 2 = reversable defect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| 52|  1|  0|     125| 212|  0|      1|    168|    0|    1.0|    2|  2|   3|     0|\n",
      "| 53|  1|  0|     140| 203|  1|      0|    155|    1|    3.1|    0|  0|   3|     0|\n",
      "| 70|  1|  0|     145| 174|  0|      1|    125|    1|    2.6|    0|  0|   3|     0|\n",
      "| 61|  1|  0|     148| 203|  0|      1|    161|    0|    0.0|    2|  1|   3|     0|\n",
      "| 62|  0|  0|     138| 294|  1|      1|    106|    0|    1.9|    1|  3|   2|     0|\n",
      "| 58|  0|  0|     100| 248|  0|      0|    122|    0|    1.0|    1|  0|   2|     1|\n",
      "| 58|  1|  0|     114| 318|  0|      2|    140|    0|    4.4|    0|  3|   1|     0|\n",
      "| 55|  1|  0|     160| 289|  0|      0|    145|    1|    0.8|    1|  1|   3|     0|\n",
      "| 46|  1|  0|     120| 249|  0|      0|    144|    0|    0.8|    2|  0|   3|     0|\n",
      "| 54|  1|  0|     122| 286|  0|      0|    116|    1|    3.2|    1|  2|   2|     0|\n",
      "| 71|  0|  0|     112| 149|  0|      1|    125|    0|    1.6|    1|  0|   2|     1|\n",
      "| 43|  0|  0|     132| 341|  1|      0|    136|    1|    3.0|    1|  0|   3|     0|\n",
      "| 34|  0|  1|     118| 210|  0|      1|    192|    0|    0.7|    2|  0|   2|     1|\n",
      "| 51|  1|  0|     140| 298|  0|      1|    122|    1|    4.2|    1|  3|   3|     0|\n",
      "| 52|  1|  0|     128| 204|  1|      1|    156|    1|    1.0|    1|  0|   0|     0|\n",
      "| 34|  0|  1|     118| 210|  0|      1|    192|    0|    0.7|    2|  0|   2|     1|\n",
      "| 51|  0|  2|     140| 308|  0|      0|    142|    0|    1.5|    2|  1|   2|     1|\n",
      "| 54|  1|  0|     124| 266|  0|      0|    109|    1|    2.2|    1|  1|   3|     0|\n",
      "| 50|  0|  1|     120| 244|  0|      1|    162|    0|    1.1|    2|  0|   2|     1|\n",
      "| 58|  1|  2|     140| 211|  1|      0|    165|    0|    0.0|    2|  0|   2|     1|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
       "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
       "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   2     3       0  \n",
       "1   0     3       0  \n",
       "2   0     3       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Úsalo con cuidado\n",
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10324029759435555"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr(\"age\", \"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+\n",
      "|         mean_age|mean_chol|\n",
      "+-----------------+---------+\n",
      "|54.43414634146342|    246.0|\n",
      "+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col as c\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df.select(\n",
    " F.avg(c(\"age\")).alias(\"mean_age\"), F.avg(c(\"chol\")).alias(\"mean_chol\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dfAgeChol = df.select(\n",
    " c(\"age\"), c(\"chol\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|max(age)|max(chol)|\n",
      "+--------+---------+\n",
      "|      77|      564|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    F.max(\"age\"),\n",
    "    F.max(\"chol\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|min(age)|min(chol)|\n",
      "+--------+---------+\n",
      "|      29|      126|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    F.min(\"age\"),\n",
    "    F.min(\"chol\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>54.43414634146342</td>\n",
       "      <td>0.6956097560975609</td>\n",
       "      <td>0.9424390243902439</td>\n",
       "      <td>131.61170731707318</td>\n",
       "      <td>246.0</td>\n",
       "      <td>0.14926829268292682</td>\n",
       "      <td>0.5297560975609756</td>\n",
       "      <td>149.11414634146342</td>\n",
       "      <td>0.33658536585365856</td>\n",
       "      <td>1.0715121951219524</td>\n",
       "      <td>1.3853658536585365</td>\n",
       "      <td>0.7541463414634146</td>\n",
       "      <td>2.32390243902439</td>\n",
       "      <td>0.5131707317073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>9.072290233244278</td>\n",
       "      <td>0.4603733241196495</td>\n",
       "      <td>1.029640743645865</td>\n",
       "      <td>17.516718005376408</td>\n",
       "      <td>51.59251020618203</td>\n",
       "      <td>0.35652668972715756</td>\n",
       "      <td>0.5278775668748918</td>\n",
       "      <td>23.00572374597721</td>\n",
       "      <td>0.4727723760037115</td>\n",
       "      <td>1.1750532551501767</td>\n",
       "      <td>0.6177552671745918</td>\n",
       "      <td>1.0307976650242825</td>\n",
       "      <td>0.6206602380510303</td>\n",
       "      <td>0.5000704980788011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>564</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>202</td>\n",
       "      <td>1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                age                 sex                  cp  \\\n",
       "0   count               1025                1025                1025   \n",
       "1    mean  54.43414634146342  0.6956097560975609  0.9424390243902439   \n",
       "2  stddev  9.072290233244278  0.4603733241196495   1.029640743645865   \n",
       "3     min                 29                   0                   0   \n",
       "4     max                 77                   1                   3   \n",
       "\n",
       "             trestbps               chol                  fbs  \\\n",
       "0                1025               1025                 1025   \n",
       "1  131.61170731707318              246.0  0.14926829268292682   \n",
       "2  17.516718005376408  51.59251020618203  0.35652668972715756   \n",
       "3                  94                126                    0   \n",
       "4                 200                564                    1   \n",
       "\n",
       "              restecg             thalach                exang  \\\n",
       "0                1025                1025                 1025   \n",
       "1  0.5297560975609756  149.11414634146342  0.33658536585365856   \n",
       "2  0.5278775668748918   23.00572374597721   0.4727723760037115   \n",
       "3                   0                  71                    0   \n",
       "4                   2                 202                    1   \n",
       "\n",
       "              oldpeak               slope                  ca  \\\n",
       "0                1025                1025                1025   \n",
       "1  1.0715121951219524  1.3853658536585365  0.7541463414634146   \n",
       "2  1.1750532551501767  0.6177552671745918  1.0307976650242825   \n",
       "3                 0.0                   0                   0   \n",
       "4                 6.2                   2                   4   \n",
       "\n",
       "                 thal              target  \n",
       "0                1025                1025  \n",
       "1    2.32390243902439  0.5131707317073171  \n",
       "2  0.6206602380510303  0.5000704980788011  \n",
       "3                   0                   0  \n",
       "4                   3                   1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ¿Qué más podemos hacer?\n",
    "\n",
    "* Casi cualquier operación tipo SQL (join, count, select, etc...)\n",
    "* ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¡Vamos a construir un random forest para resolver un problema de clasificación!\n",
    "\n",
    "**Nota:** Para este caso vamos a obviar varios pasos que son esenciales: limpieza de datos, análisis exploratorios, análisis de relevancia de variables, etc. El único fin es mostrar que se puede efectuar los pasos para aplicar un Random Forest a un set de datos.\n",
    "\n",
    "<img src='https://www.researchgate.net/profile/Maninder_Kaur47/publication/333152684/figure/fig3/AS:767795625218049@1560068150841/Random-forest-simplified-representation.jpg' alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "*Random forest* = Ensamble de árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Tree](https://media.geeksforgeeks.org/wp-content/cdn-uploads/Decision_Tree-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## a. Antes que nada, en Spark tenemos que ensamblar nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[52.0, 1.0, 0.0, 125.0, 212.0, 0.0, 1.0, 168.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[53.0, 1.0, 0.0, 140.0, 203.0, 1.0, 0.0, 155.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[70.0, 1.0, 0.0, 145.0, 174.0, 0.0, 1.0, 125.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  target\n",
       "0  [52.0, 1.0, 0.0, 125.0, 212.0, 0.0, 1.0, 168.0...       0\n",
       "1  [53.0, 1.0, 0.0, 140.0, 203.0, 1.0, 0.0, 155.0...       0\n",
       "2  [70.0, 1.0, 0.0, 145.0, 174.0, 0.0, 1.0, 125.0...       0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "features = df.columns\n",
    "features.remove(\"target\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols = features,outputCol='features')\n",
    "\n",
    "assembled = assembler.transform(df)\n",
    "\n",
    "#Sólo VA y target\n",
    "data_VA = assembled.select('features','target')\n",
    "\n",
    "data_VA.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## b. Si hay datos categóricos convertirlos en numéricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## c. Partir los datos en un conjunto de prueba y uno de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = data_VA.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## d. Agregar los imports necesarios para Random Forest\n",
    "\n",
    "https://github.com/LiberPH/taller_spark/blob/master/random_forest/RandomForest.ipynb\n",
    "    \n",
    "https://github.com/datadogmx/taller_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## e. Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"target\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Train model.\n",
    "model = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## f. Probar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|prediction|         probability|            features|\n",
      "+----------+--------------------+--------------------+\n",
      "|       1.0|[0.12315371192873...|(13,[0,1,3,4,7,10...|\n",
      "|       1.0|[0.12315371192873...|(13,[0,1,3,4,7,10...|\n",
      "|       1.0|[0.22325738961306...|(13,[0,1,3,4,7,10...|\n",
      "|       1.0|[0.22325738961306...|(13,[0,1,3,4,7,10...|\n",
      "|       1.0|[0.12841686982347...|(13,[0,3,4,6,7,10...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"probability\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## g. Ver algunas métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0811688\n"
     ]
    }
   ],
   "source": [
    "# Compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>target</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(45.0, 1.0, 0.0, 115.0, 260.0, 0.0, 0.0, 185.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.2315371192873732, 8.768462880712628]</td>\n",
       "      <td>[0.12315371192873732, 0.8768462880712627]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(48.0, 1.0, 0.0, 122.0, 222.0, 0.0, 0.0, 186.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.2315371192873732, 8.768462880712628]</td>\n",
       "      <td>[0.12315371192873732, 0.8768462880712627]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(59.0, 1.0, 0.0, 138.0, 271.0, 0.0, 0.0, 182.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2.2325738961306016, 7.7674261038694]</td>\n",
       "      <td>[0.22325738961306013, 0.7767426103869398]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(59.0, 1.0, 0.0, 138.0, 271.0, 0.0, 0.0, 182.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2.2325738961306016, 7.7674261038694]</td>\n",
       "      <td>[0.22325738961306013, 0.7767426103869398]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(49.0, 0.0, 0.0, 130.0, 269.0, 0.0, 1.0, 163.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.2841686982347416, 8.71583130176526]</td>\n",
       "      <td>[0.12841686982347414, 0.8715831301765258]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(46.0, 0.0, 0.0, 138.0, 243.0, 0.0, 0.0, 152.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[4.664545820122207, 5.335454179877794]</td>\n",
       "      <td>[0.4664545820122207, 0.5335454179877794]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(46.0, 0.0, 0.0, 138.0, 243.0, 0.0, 0.0, 152.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[4.664545820122207, 5.335454179877794]</td>\n",
       "      <td>[0.4664545820122207, 0.5335454179877794]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(42.0, 0.0, 0.0, 102.0, 265.0, 0.0, 0.0, 122.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2.1274422979441336, 7.872557702055866]</td>\n",
       "      <td>[0.21274422979441338, 0.7872557702055867]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(53.0, 0.0, 0.0, 130.0, 264.0, 0.0, 0.0, 143.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2.904029552288442, 7.095970447711559]</td>\n",
       "      <td>[0.2904029552288442, 0.7095970447711559]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(53.0, 0.0, 0.0, 130.0, 264.0, 0.0, 0.0, 143.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2.904029552288442, 7.095970447711559]</td>\n",
       "      <td>[0.2904029552288442, 0.7095970447711559]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  target  \\\n",
       "0  (45.0, 1.0, 0.0, 115.0, 260.0, 0.0, 0.0, 185.0...       1   \n",
       "1  (48.0, 1.0, 0.0, 122.0, 222.0, 0.0, 0.0, 186.0...       1   \n",
       "2  (59.0, 1.0, 0.0, 138.0, 271.0, 0.0, 0.0, 182.0...       1   \n",
       "3  (59.0, 1.0, 0.0, 138.0, 271.0, 0.0, 0.0, 182.0...       1   \n",
       "4  (49.0, 0.0, 0.0, 130.0, 269.0, 0.0, 1.0, 163.0...       1   \n",
       "5  (46.0, 0.0, 0.0, 138.0, 243.0, 0.0, 0.0, 152.0...       1   \n",
       "6  (46.0, 0.0, 0.0, 138.0, 243.0, 0.0, 0.0, 152.0...       1   \n",
       "7  (42.0, 0.0, 0.0, 102.0, 265.0, 0.0, 0.0, 122.0...       1   \n",
       "8  (53.0, 0.0, 0.0, 130.0, 264.0, 0.0, 0.0, 143.0...       1   \n",
       "9  (53.0, 0.0, 0.0, 130.0, 264.0, 0.0, 0.0, 143.0...       1   \n",
       "\n",
       "                             rawPrediction  \\\n",
       "0  [1.2315371192873732, 8.768462880712628]   \n",
       "1  [1.2315371192873732, 8.768462880712628]   \n",
       "2    [2.2325738961306016, 7.7674261038694]   \n",
       "3    [2.2325738961306016, 7.7674261038694]   \n",
       "4   [1.2841686982347416, 8.71583130176526]   \n",
       "5   [4.664545820122207, 5.335454179877794]   \n",
       "6   [4.664545820122207, 5.335454179877794]   \n",
       "7  [2.1274422979441336, 7.872557702055866]   \n",
       "8   [2.904029552288442, 7.095970447711559]   \n",
       "9   [2.904029552288442, 7.095970447711559]   \n",
       "\n",
       "                                 probability  prediction  \n",
       "0  [0.12315371192873732, 0.8768462880712627]         1.0  \n",
       "1  [0.12315371192873732, 0.8768462880712627]         1.0  \n",
       "2  [0.22325738961306013, 0.7767426103869398]         1.0  \n",
       "3  [0.22325738961306013, 0.7767426103869398]         1.0  \n",
       "4  [0.12841686982347414, 0.8715831301765258]         1.0  \n",
       "5   [0.4664545820122207, 0.5335454179877794]         1.0  \n",
       "6   [0.4664545820122207, 0.5335454179877794]         1.0  \n",
       "7  [0.21274422979441338, 0.7872557702055867]         1.0  \n",
       "8   [0.2904029552288442, 0.7095970447711559]         1.0  \n",
       "9   [0.2904029552288442, 0.7095970447711559]         1.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alguien que haya usado Scikit Learn probablemente dirá que Spark es complicadísimo. ¿Por qué querría yo usarlo existiendo algo tan hermoso como Scikit Learn?\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/-YmyKgTo9QY/hqdefault.jpg\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Errores o detalles comunes \n",
    "\n",
    "Supongamos que tengo un caso en el que vale la pena usar Spark y esta disponible un clúster con ya todo instalado para usarlo.\n",
    "\n",
    "<img src='https://miro.medium.com/max/10368/0*46YPu9w09V1r_3th' alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "¿Qué debería tener en cuenta?\n",
    "https://sparkbyexamples.com/spark/spark-performance-tuning/\n",
    "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shuffling\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/199Q1.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://i.stack.imgur.com/4jbD6.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/844/0*kAw8hogu1oZPy9QU.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "https://medium.com/@dvcanton/wide-and-narrow-dependencies-in-apache-spark-21acf2faf031"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Joins no balanceados\n",
    "\n",
    "Una forma de evitar hacer shuffling cuando los joins no están balanceados y un data frame es muy pequeño es hacer un broadcast join.\n",
    "\n",
    "<img src=\"https://www.oreilly.com/library/view/high-performance-spark/9781491943199/assets/hpsp_0405.png\" alt=\"drawing\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>chol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia T</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gabriela D</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrea M</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Luis P</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name  age  sex  chol\n",
       "0    Olivia T   52    1   212\n",
       "1  Gabriela D   70    1   174\n",
       "2    Andrea M   54    1   286\n",
       "3      Luis P   34    0   210"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "patients = {\"Name\": [\"Olivia T\",\"Gabriela D\",\"Andrea M\",\"Luis P\"],\n",
    "        \"age\": [52,70,54,34],\n",
    "        \"sex\": [1,1,1,0],\n",
    "        \"chol\": [212,174,286,210],\n",
    "        }\n",
    "\n",
    "patientsPd = pd.DataFrame(patients, columns = ['Name', 'age', 'sex', 'chol'])\n",
    "patientsPd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Funciona en Spark 3.0\n",
    "patientsSpark = spark.createDataFrame(patientsPd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "joined = patientsSpark.join(F.broadcast(df), ((patientsSpark.age == df.age) & (patientsSpark.sex == df.sex) &\n",
    "        ( patientsSpark.chol == df.chol)), 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>chol</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia T</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>212</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Olivia T</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>212</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Olivia T</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>212</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Olivia T</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>212</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gabriela D</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gabriela D</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gabriela D</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gabriela D</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Andrea M</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>286</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Andrea M</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>286</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name  age  sex  chol  age  sex  cp  trestbps  chol  fbs  restecg  \\\n",
       "0    Olivia T   52    1   212   52    1   0       125   212    0        1   \n",
       "1    Olivia T   52    1   212   52    1   0       125   212    0        1   \n",
       "2    Olivia T   52    1   212   52    1   0       125   212    0        1   \n",
       "3    Olivia T   52    1   212   52    1   0       125   212    0        1   \n",
       "4  Gabriela D   70    1   174   70    1   0       145   174    0        1   \n",
       "5  Gabriela D   70    1   174   70    1   0       145   174    0        1   \n",
       "6  Gabriela D   70    1   174   70    1   0       145   174    0        1   \n",
       "7  Gabriela D   70    1   174   70    1   0       145   174    0        1   \n",
       "8    Andrea M   54    1   286   54    1   0       122   286    0        0   \n",
       "9    Andrea M   54    1   286   54    1   0       122   286    0        0   \n",
       "\n",
       "   thalach  exang  oldpeak  slope  ca  thal  target  \n",
       "0      168      0      1.0      2   2     3       0  \n",
       "1      168      0      1.0      2   2     3       0  \n",
       "2      168      0      1.0      2   2     3       0  \n",
       "3      168      0      1.0      2   2     3       0  \n",
       "4      125      1      2.6      0   0     3       0  \n",
       "5      125      1      2.6      0   0     3       0  \n",
       "6      125      1      2.6      0   0     3       0  \n",
       "7      125      1      2.6      0   0     3       0  \n",
       "8      116      1      3.2      1   2     2       0  \n",
       "9      116      1      3.2      1   2     2       0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+-----+\n",
      "|age|sex|chol|count|\n",
      "+---+---+----+-----+\n",
      "| 38|  1| 175|    8|\n",
      "| 54|  1| 239|    7|\n",
      "| 43|  1| 247|    6|\n",
      "| 71|  0| 149|    4|\n",
      "| 71|  0| 265|    4|\n",
      "| 70|  1| 322|    4|\n",
      "| 70|  1| 174|    4|\n",
      "| 67|  1| 286|    4|\n",
      "| 67|  1| 212|    4|\n",
      "| 67|  1| 229|    4|\n",
      "| 67|  1| 237|    4|\n",
      "| 66|  0| 278|    4|\n",
      "| 66|  1| 212|    4|\n",
      "| 66|  1| 228|    4|\n",
      "| 66|  0| 228|    4|\n",
      "| 65|  1| 282|    4|\n",
      "| 65|  1| 254|    4|\n",
      "| 65|  0| 225|    4|\n",
      "| 64|  1| 246|    4|\n",
      "| 64|  1| 335|    4|\n",
      "+---+---+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\").groupBy(\"age\",\"sex\",\"chol\").count().orderBy(\"count\",\"age\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![RDD vs dataframes](https://databricks.com/wp-content/uploads/2016/07/memory-usage-when-caching-datasets-vs-rdds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cuidado con cómo se particionan los sets de datos\n",
    "\n",
    "* Esto aplica para la escritura en disco de los data sets\n",
    "* Un exceso de particiones puede hacer la lectura muy lenta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recursos\n",
    "\n",
    "* [Rock the JVM Spark courses (Scala)](https://rockthejvm.com/)\n",
    "* [About Spark in mapr](https://mapr.com/blog/spark-101-what-it-what-it-does-and-why-it-matters/)\n",
    "* [edX course Big Data analytics Using Spark](https://courses.edx.org/courses/course-v1:UCSanDiegoX+DSE230x+1T2018/courseware/2966295eefb4492b836de9ee34f05911/56608329f8fc42fc8fcaa5e49673ee3b/5?activate_block_id=block-v1%3AUCSanDiegoX%2BDSE230x%2B1T2018%2Btype%40vertical%2Bblock%4042ff48e505e24071b920a496b48a32bc)\n",
    "* [Spark-py notebooks by Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)\n",
    "* [Presenting code using Jupyter Notebook Slides](https://medium.com/@mjspeck/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67)\n",
    "* http://dme.rwth-aachen.de/en/research/projects/mapreduce\n",
    "* https://softwareengineering.stackexchange.com/questions/98800/is-mapreduce-anything-more-than-just-an-application-of-divide-and-conquer\n",
    "* http://data-flair.training/forums/topic/sparksession-vs-sparkcontext-in-apache-spark"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
